{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Ensemble Anomaly Detection\n\n**Мета:** Об'єднати результати всіх методів в єдиний risk score.\n\n**Методи (5 рівнів):**\n1. Rule-based (44 rules) — процедурні порушення\n2. Statistical (Benford, Z-score) — числові аномалії\n3. Isolation Forest — глобальні outliers\n4. HDBSCAN — кластеризація + outlier detection\n5. Network Analysis — мережевий аналіз (картелі, монополії)\n\n**Ensemble підхід:**\n- Weighted voting: кожен метод голосує з вагою\n- Consensus: скільки методів flagged тендер\n- Final risk score: нормалізована комбінація\n\n**Pipeline:**\n1. Rule-based ✓\n2. Statistical ✓\n3. Isolation Forest ✓\n4. HDBSCAN ✓\n5. Network ✓\n6. **Ensemble** ← current"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '..')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\n\nfrom src.data_loader import load_tenders, load_bids, load_buyers, load_suppliers, load_bidders\nfrom src.detectors import (\n    RuleBasedDetector,\n    StatisticalDetector,\n    IsolationForestDetector,\n    HDBSCANDetector,\n    NetworkAnalysisDetector,\n    EnsembleDetector,\n)\n\n# ============================================================\n# CONFIGURATION\n# ============================================================\nYEARS = [2022, 2023, 2024, 2025]\nCONTAMINATION = 0.05\nHDBSCAN_SAMPLE_SIZE = 500_000  # HDBSCAN on sample\n# ============================================================\n\n# Create output directories\nPath('../results/figures/ensemble').mkdir(parents=True, exist_ok=True)\nPath('../results').mkdir(parents=True, exist_ok=True)\n\n# Style\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['font.size'] = 11\n\nprint(f\"Configuration: YEARS={YEARS}, CONTAMINATION={CONTAMINATION}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Завантаження даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "print(\"Loading data...\")\ntenders = load_tenders(years=YEARS)\nbids = load_bids(years=YEARS)\nbuyers = load_buyers()\nsuppliers = load_suppliers()\nbidders = load_bidders()\n\nprint(f\"\\nDataset:\")\nprint(f\"  Tenders: {len(tenders):,}\")\nprint(f\"  Bids: {len(bids):,}\")\nprint(f\"  Buyers: {len(buyers):,}\")\nprint(f\"  Suppliers: {len(suppliers):,}\")\nprint(f\"  Bidders: {len(bidders):,}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Run All Detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Rule-based\n",
    "print(\"=\"*60)\n",
    "print(\"1. RULE-BASED DETECTOR\")\n",
    "print(\"=\"*60)\n",
    "rule_detector = RuleBasedDetector()\n",
    "rule_results = rule_detector.detect(tenders, buyers_df=buyers, bids_df=bids)\n",
    "print(f\"\\nRule-based anomalies (score >= 6): {(rule_results['rule_risk_score'] >= 6).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Statistical\n",
    "print(\"=\"*60)\n",
    "print(\"2. STATISTICAL DETECTOR\")\n",
    "print(\"=\"*60)\n",
    "stat_detector = StatisticalDetector()\n",
    "stat_results = stat_detector.detect(tenders, bids_df=bids)\n",
    "print(f\"\\nStatistical anomalies (score >= 3): {(stat_results['stat_score'] >= 3).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# 3. Isolation Forest\nprint(\"=\"*60)\nprint(\"3. ISOLATION FOREST\")\nprint(\"=\"*60)\n\n# Features for ML methods\nfeatures_ml = {\n    \"tender\": [\n        \"tender_value\", \"price_change_pct\", \"number_of_tenderers\",\n        \"is_single_bidder\", \"is_competitive\",\n        \"is_weekend\", \"is_q4\", \"is_december\",\n    ],\n    \"buyer\": [\n        \"single_bidder_rate\", \"competitive_rate\",\n        \"avg_discount_pct\", \"supplier_diversity_index\",\n    ],\n    \"supplier\": [\"total_awards\", \"total_value\"],\n}\n\nif_detector = IsolationForestDetector(\n    contamination=CONTAMINATION,\n    n_estimators=100,\n    random_state=42,\n    features=features_ml,\n)\nif_results = if_detector.fit_detect(tenders, buyers_df=buyers, suppliers_df=suppliers)\nprint(f\"\\nIF anomalies: {if_results['if_anomaly'].sum():,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# 4. HDBSCAN (on sample)\nprint(\"=\"*60)\nprint(\"4. HDBSCAN (sample)\")\nprint(\"=\"*60)\n\nhdbscan_detector = HDBSCANDetector(\n    min_cluster_size=50,\n    min_samples=10,\n    contamination=CONTAMINATION,\n    features=features_ml,\n)\nhdbscan_results = hdbscan_detector.fit_detect(\n    tenders, \n    buyers_df=buyers, \n    suppliers_df=suppliers,\n    sample_size=HDBSCAN_SAMPLE_SIZE,\n)\nprint(f\"\\nHDBSCAN anomalies: {hdbscan_results['hdbscan_anomaly'].sum():,}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "# 5. Network Analysis\nprint(\"=\"*60)\nprint(\"5. NETWORK ANALYSIS\")\nprint(\"=\"*60)\n\nnetwork_detector = NetworkAnalysisDetector(\n    min_co_bids=3,\n    min_contracts=3,\n    min_community_size=3,\n)\nnetwork_results = network_detector.fit_detect(tenders, bids_df=bids, bidders_df=bidders)\nprint(f\"\\nNetwork anomalies: {network_results['network_anomaly'].sum():,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# Combine all results using EnsembleDetector\nprint(\"=\"*60)\nprint(\"COMBINING RESULTS WITH ENSEMBLE DETECTOR\")\nprint(\"=\"*60)\n\nensemble_detector = EnsembleDetector(\n    weights={\n        'rule': 1.0,\n        'stat': 0.8,\n        'if': 1.0,\n        'hdbscan': 0.8,\n        'network': 1.0,\n    },\n    consensus_threshold=2,\n)\n\nensemble = ensemble_detector.combine(\n    rule_results=rule_results,\n    stat_results=stat_results,\n    if_results=if_results,\n    hdbscan_results=hdbscan_results,\n    network_results=network_results,\n)\n\nprint(f\"\\nEnsemble dataset: {len(ensemble):,} tenders\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "# Skip - EnsembleDetector handles normalization internally"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# Scores are already normalized by EnsembleDetector\nprint(\"Score columns in ensemble results:\")\nprint([col for col in ensemble.columns if 'score' in col])"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Compute Ensemble Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# EnsembleDetector computes scores and consensus automatically\n# ensemble_score, consensus_count, consensus_pct, ensemble_anomaly, ensemble_risk_level\n\nprint(\"WEIGHTS:\", ensemble_detector.weights)\nprint(f\"\\nEnsemble score stats:\")\nprint(ensemble['ensemble_score'].describe())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "# Risk levels are computed by EnsembleDetector\n# Based on consensus_count: critical (>=75%), high (>=50%), medium (>=25%), low (<25%)\n\nrisk_dist = ensemble['ensemble_risk_level'].value_counts()\nprint(\"\\nENSEMBLE RISK DISTRIBUTION:\")\nfor level in ['critical', 'high', 'medium', 'low']:\n    count = risk_dist.get(level, 0)\n    pct = count / len(ensemble) * 100\n    print(f\"  {level:10} {count:>10,} ({pct:>5.2f}%)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Consensus Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "# Consensus breakdown (5 methods)\nn_methods = len(ensemble_detector.methods_used)\nprint(f\"CONSENSUS BREAKDOWN ({n_methods} methods: {ensemble_detector.methods_used}):\")\n\nconsensus_dist = ensemble['consensus_count'].value_counts().sort_index()\nfor count, num in consensus_dist.items():\n    pct = num / len(ensemble) * 100\n    methods = f\"{count}/{n_methods} methods\"\n    print(f\"  {methods}: {num:>10,} ({pct:>5.2f}%)\")\n\n# Critical: most methods agree\ncritical_threshold = int(n_methods * 0.75)  # 75%+ methods\ncritical_consensus = ensemble[ensemble['consensus_count'] >= critical_threshold]\nprint(f\"\\nCRITICAL ({critical_threshold}+ of {n_methods} methods): {len(critical_consensus):,} tenders\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize consensus\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Consensus distribution\nn_methods = len(ensemble_detector.methods_used)\ncolors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, n_methods + 1))\naxes[0].bar(consensus_dist.index, consensus_dist.values, color=colors[:len(consensus_dist)])\naxes[0].set_xlabel('Number of Methods Flagged')\naxes[0].set_ylabel('Number of Tenders')\naxes[0].set_title(f'Consensus Distribution ({n_methods} methods)')\naxes[0].set_xticks(range(n_methods + 1))\n\n# Risk level pie\nrisk_colors = {'critical': '#d62728', 'high': '#ff7f0e', 'medium': '#ffbb78', 'low': '#2ca02c'}\nrisk_order = ['critical', 'high', 'medium', 'low']\nrisk_values = [risk_dist.get(r, 0) for r in risk_order]\naxes[1].pie(risk_values, labels=risk_order, colors=[risk_colors[r] for r in risk_order],\n            autopct='%1.1f%%', startangle=90)\naxes[1].set_title('Ensemble Risk Distribution')\n\nplt.tight_layout()\nplt.savefig('../results/figures/ensemble/consensus_distribution.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. Method Contribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "# Method contribution analysis using EnsembleDetector\nprint(\"METHOD STATISTICS:\")\nprint(ensemble_detector.method_summary().to_string(index=False))\n\n# Correlation matrix\nprint(\"\\nSCORE CORRELATION MATRIX:\")\ncorr_matrix = ensemble_detector.correlation_matrix()\nprint(corr_matrix.round(3).to_string())\n\n# Agreement matrix (Jaccard similarity)\nprint(\"\\nANOMALY AGREEMENT MATRIX (Jaccard):\")\nagreement_matrix = ensemble_detector.agreement_matrix()\nprint(agreement_matrix.round(3).to_string())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize agreement matrix\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Correlation heatmap\nax = axes[0]\nsns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax)\nax.set_title('Score Correlation Matrix')\n\n# Agreement heatmap (Jaccard)\nax = axes[1]\nsns.heatmap(agreement_matrix.astype(float), annot=True, fmt='.2f', cmap='YlOrRd', ax=ax)\nax.set_title('Anomaly Agreement Matrix (Jaccard)')\n\nplt.tight_layout()\nplt.savefig('../results/figures/ensemble/method_agreement.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 8. Critical Tenders Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": "# Get critical tenders (using EnsembleDetector helper)\ncritical_tenders = ensemble_detector.get_critical_tenders()\nnormal_tenders = ensemble[ensemble['consensus_count'] == 0]\n\n# Merge with tender details\ncritical_with_details = critical_tenders.merge(\n    tenders[['tender_id', 'tender_value', 'is_single_bidder', 'is_competitive']],\n    on='tender_id', how='left'\n)\nnormal_with_details = normal_tenders.merge(\n    tenders[['tender_id', 'tender_value', 'is_single_bidder', 'is_competitive']],\n    on='tender_id', how='left'\n)\n\nprint(\"=\"*60)\nprint(\"CRITICAL TENDERS CHARACTERISTICS\")\nprint(\"=\"*60)\n\nprint(f\"\\n{'Metric':<30} {'Critical':>15} {'Normal':>15}\")\nprint(\"-\"*60)\n\n# Value\nc_mean = critical_with_details['tender_value'].mean()\nn_mean = normal_with_details['tender_value'].mean()\nprint(f\"{'Mean tender value (M UAH)':<30} {c_mean/1e6:>15,.2f} {n_mean/1e6:>15,.2f}\")\n\nc_med = critical_with_details['tender_value'].median()\nn_med = normal_with_details['tender_value'].median()\nprint(f\"{'Median tender value (K UAH)':<30} {c_med/1e3:>15,.1f} {n_med/1e3:>15,.1f}\")\n\n# Competition\nc_sb = critical_with_details['is_single_bidder'].mean()\nn_sb = normal_with_details['is_single_bidder'].mean()\nprint(f\"{'Single bidder rate (%)':<30} {c_sb*100:>15.1f} {n_sb*100:>15.1f}\")\n\nc_comp = critical_with_details['is_competitive'].mean()\nn_comp = normal_with_details['is_competitive'].mean()\nprint(f\"{'Competitive rate (%)':<30} {c_comp*100:>15.1f} {n_comp*100:>15.1f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "# Procurement method distribution\nprint(\"\\nPROCUREMENT METHOD:\")\n\n# Merge with tenders to get procurement method\ncritical_methods = critical_tenders.merge(tenders[['tender_id', 'procurement_method']], on='tender_id')\nnormal_methods = normal_tenders.merge(tenders[['tender_id', 'procurement_method']], on='tender_id')\n\ncritical_method_dist = critical_methods['procurement_method'].value_counts(normalize=True) * 100\nnormal_method_dist = normal_methods['procurement_method'].value_counts(normalize=True) * 100\n\nfor method in critical_method_dist.index:\n    c_pct = critical_method_dist.get(method, 0)\n    n_pct = normal_method_dist.get(method, 0)\n    ratio = c_pct / n_pct if n_pct > 0 else 0\n    print(f\"  {method}: {c_pct:.1f}% (vs {n_pct:.1f}% normal) - {ratio:.1f}x\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": "# Year distribution\nprint(\"\\nYEAR DISTRIBUTION:\")\n\ncritical_years = critical_tenders.merge(tenders[['tender_id', 'year']], on='tender_id')\ncritical_year_dist = critical_years['year'].value_counts().sort_index()\n\nfor year, count in critical_year_dist.items():\n    total_year = len(tenders[tenders['year'] == year])\n    pct = count / total_year * 100\n    print(f\"  {year}: {count:,} ({pct:.2f}% of year)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 9. Top Risky Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": "# Top buyers by critical tender count\nprint(\"TOP 10 BUYERS BY CRITICAL TENDERS:\")\n\ncritical_full = critical_tenders.merge(\n    tenders[['tender_id', 'buyer_id', 'supplier_id', 'tender_value']], \n    on='tender_id', how='left'\n)\n\ntop_buyers_critical = critical_full.groupby('buyer_id').agg({\n    'tender_id': 'count',\n    'tender_value': 'sum',\n    'ensemble_score': 'mean'\n}).sort_values('tender_id', ascending=False).head(10)\n\ntop_buyers_critical = top_buyers_critical.reset_index().merge(\n    buyers[['buyer_id', 'buyer_name', 'buyer_region']], on='buyer_id', how='left'\n)\n\nfor _, row in top_buyers_critical.iterrows():\n    name = str(row['buyer_name'])[:50] if pd.notna(row['buyer_name']) else 'N/A'\n    print(f\"  {row['tender_id']:>5,} tenders | {row['tender_value']/1e6:>10,.1f}M UAH | {name}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": "# Top suppliers by critical tender count\nprint(\"\\nTOP 10 SUPPLIERS BY CRITICAL TENDERS:\")\n\ntop_suppliers_critical = critical_full.groupby('supplier_id').agg({\n    'tender_id': 'count',\n    'tender_value': 'sum',\n    'ensemble_score': 'mean'\n}).sort_values('tender_id', ascending=False).head(10)\n\ntop_suppliers_critical = top_suppliers_critical.reset_index().merge(\n    suppliers[['supplier_id', 'supplier_name']], on='supplier_id', how='left'\n)\n\nfor _, row in top_suppliers_critical.iterrows():\n    name = str(row['supplier_name'])[:50] if pd.notna(row['supplier_name']) else 'N/A'\n    print(f\"  {row['tender_id']:>5,} tenders | {row['tender_value']/1e6:>10,.1f}M UAH | {name}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": "# Save ensemble results\nprint(\"Saving results...\")\n\n# Define output columns\nscore_cols = [col for col in ensemble.columns if 'score' in col]\nanomaly_cols = [col for col in ensemble.columns if 'anomaly' in col]\noutput_cols = ['tender_id'] + score_cols + anomaly_cols + ['consensus_count', 'consensus_pct', 'ensemble_risk_level']\n\n# Save full results\nensemble[output_cols].to_csv('../results/ensemble_results.csv', index=False)\nprint(f\"Saved full results: results/ensemble_results.csv ({len(ensemble):,} rows)\")\n\n# Save critical only\ncritical_tenders[output_cols].to_csv('../results/critical_tenders.csv', index=False)\nprint(f\"Saved critical tenders: results/critical_tenders.csv ({len(critical_tenders):,} rows)\")\n\n# Save summary\nsummary = ensemble_detector.summary()\nsummary.to_csv('../results/ensemble_summary.csv', index=False)\nprint(f\"Saved summary: results/ensemble_summary.csv\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"ENSEMBLE SUMMARY\")\nprint(\"=\"*60)\n\nprint(f\"\\nDataset: {len(ensemble):,} tenders ({YEARS[0]}-{YEARS[-1]})\")\nprint(f\"\\nMethods combined ({len(ensemble_detector.methods_used)}):\")\nfor method in ensemble_detector.methods_used:\n    anomaly_col = f\"{method}_anomaly\"\n    if anomaly_col in ensemble.columns:\n        count = ensemble[anomaly_col].sum()\n        print(f\"  {method}: {count:,} anomalies\")\n\nprint(f\"\\nENSEMBLE RISK LEVELS:\")\nfor level in ['critical', 'high', 'medium', 'low']:\n    count = risk_dist.get(level, 0)\n    pct = count / len(ensemble) * 100\n    print(f\"  {level:10} {count:>10,} ({pct:>5.2f}%)\")\n\nprint(f\"\\nCRITICAL TENDERS (ensemble_risk_level == 'critical'):\")\nprint(f\"  Count: {len(critical_tenders):,}\")\nif len(critical_with_details) > 0:\n    print(f\"  Total value: {critical_with_details['tender_value'].sum()/1e9:.2f}B UAH\")\n    print(f\"  Mean value: {critical_with_details['tender_value'].mean()/1e6:.2f}M UAH\")\n    print(f\"  Single bidder rate: {critical_with_details['is_single_bidder'].mean()*100:.1f}%\")\n\nprint(\"\\n\" + \"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": "## Висновки\n\n### Ensemble підхід (5 методів):\n- **Rule-based** — процедурні порушення (44 rules)\n- **Statistical** — числові аномалії (Benford, Z-score, HHI)\n- **Isolation Forest** — глобальні outliers\n- **HDBSCAN** — кластеризація + outlier detection\n- **Network Analysis** — картелі, монополії, bid rotation\n\n### Scoring:\n- **Weighted score**: зважена комбінація нормалізованих scores\n- **Consensus voting**: кількість методів, що flagged тендер\n- **Risk levels**: critical (>=75%), high (>=50%), medium (>=25%), low (<25%)\n\n### Ключові результати:\n- **Critical** (більшість методів згодні) — найвища впевненість для аудиту\n- Методи доповнюють один одного (низька кореляція)\n- Різні типи аномалій покриваються різними методами\n\n### Збережені файли:\n- `results/ensemble_results.csv` — всі тендери з scores\n- `results/critical_tenders.csv` — лише critical\n- `results/ensemble_summary.csv` — summary statistics\n\n### EnsembleDetector:\n```python\nfrom src.detectors import EnsembleDetector\n\ndetector = EnsembleDetector(consensus_threshold=2)\nresults = detector.combine(\n    rule_results=...,\n    stat_results=...,\n    if_results=...,\n    hdbscan_results=...,\n    network_results=...,\n)\n\ncritical = detector.get_critical_tenders()\nprint(detector.summary())\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}