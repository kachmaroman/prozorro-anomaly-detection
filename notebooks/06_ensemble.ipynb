{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Ensemble Anomaly Detection\n",
    "\n",
    "**Мета:** Об'єднати результати всіх методів в єдиний risk score.\n",
    "\n",
    "**Методи:**\n",
    "1. Rule-based (44 rules) — процедурні порушення\n",
    "2. Statistical (Benford, Z-score) — числові аномалії\n",
    "3. Isolation Forest — глобальні outliers\n",
    "4. LOF — локальні outliers\n",
    "\n",
    "**Ensemble підхід:**\n",
    "- Weighted voting: кожен метод голосує з вагою\n",
    "- Consensus: скільки методів flagged тендер\n",
    "- Final risk score: нормалізована комбінація\n",
    "\n",
    "**Pipeline:**\n",
    "1. Rule-based ✓\n",
    "2. Statistical ✓\n",
    "3. Isolation Forest ✓\n",
    "4. LOF ✓\n",
    "5. **Ensemble** ← current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data_loader import load_tenders, load_bids, load_buyers, load_suppliers\n",
    "from src.detectors.rule_based import RuleBasedDetector\n",
    "from src.detectors.statistical import StatisticalDetector\n",
    "from src.detectors.ml_based import IsolationForestDetector, LOFDetector\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "YEARS = [2022, 2023, 2024, 2025]\n",
    "CONTAMINATION = 0.05\n",
    "LOF_SAMPLE_SIZE = 500_000  # LOF on sample, then extrapolate\n",
    "# ============================================================\n",
    "\n",
    "# Create output directories\n",
    "Path('../results/figures/ensemble').mkdir(parents=True, exist_ok=True)\n",
    "Path('../results').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(f\"Configuration: YEARS={YEARS}, CONTAMINATION={CONTAMINATION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Завантаження даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "tenders = load_tenders(years=YEARS)\n",
    "bids = load_bids(years=YEARS)\n",
    "buyers = load_buyers()\n",
    "suppliers = load_suppliers()\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Tenders: {len(tenders):,}\")\n",
    "print(f\"  Bids: {len(bids):,}\")\n",
    "print(f\"  Buyers: {len(buyers):,}\")\n",
    "print(f\"  Suppliers: {len(suppliers):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Run All Detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Rule-based\n",
    "print(\"=\"*60)\n",
    "print(\"1. RULE-BASED DETECTOR\")\n",
    "print(\"=\"*60)\n",
    "rule_detector = RuleBasedDetector()\n",
    "rule_results = rule_detector.detect(tenders, buyers_df=buyers, bids_df=bids)\n",
    "print(f\"\\nRule-based anomalies (score >= 6): {(rule_results['rule_risk_score'] >= 6).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Statistical\n",
    "print(\"=\"*60)\n",
    "print(\"2. STATISTICAL DETECTOR\")\n",
    "print(\"=\"*60)\n",
    "stat_detector = StatisticalDetector()\n",
    "stat_results = stat_detector.detect(tenders, bids_df=bids)\n",
    "print(f\"\\nStatistical anomalies (score >= 3): {(stat_results['stat_score'] >= 3).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Isolation Forest\n",
    "print(\"=\"*60)\n",
    "print(\"3. ISOLATION FOREST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extended features\n",
    "features_extended = {\n",
    "    \"tender\": [\n",
    "        \"tender_value\", \"price_change_pct\", \"number_of_tenderers\",\n",
    "        \"is_single_bidder\", \"is_competitive\",\n",
    "        \"is_weekend\", \"is_q4\", \"is_december\",\n",
    "    ],\n",
    "    \"buyer\": [\n",
    "        \"single_bidder_rate\", \"competitive_rate\",\n",
    "        \"avg_discount_pct\", \"supplier_diversity_index\",\n",
    "    ],\n",
    "    \"supplier\": [\"total_awards\", \"total_value\"],\n",
    "}\n",
    "\n",
    "if_detector = IsolationForestDetector(\n",
    "    contamination=CONTAMINATION,\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    features=features_extended,\n",
    ")\n",
    "if_results = if_detector.fit_detect(tenders, buyers_df=buyers, suppliers_df=suppliers)\n",
    "print(f\"\\nIF anomalies: {if_results['if_anomaly'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. LOF (on sample)\n",
    "print(\"=\"*60)\n",
    "print(\"4. LOCAL OUTLIER FACTOR (sample)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sample for LOF\n",
    "tenders_sample = tenders.sample(LOF_SAMPLE_SIZE, random_state=42)\n",
    "print(f\"LOF sample size: {len(tenders_sample):,}\")\n",
    "\n",
    "lof_detector = LOFDetector(\n",
    "    n_neighbors=20,\n",
    "    contamination=CONTAMINATION,\n",
    "    features=features_extended,\n",
    ")\n",
    "lof_results = lof_detector.fit_detect(tenders_sample, buyers_df=buyers, suppliers_df=suppliers)\n",
    "print(f\"\\nLOF anomalies: {lof_results['lof_anomaly'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Merge All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all results\n",
    "print(\"Merging results...\")\n",
    "\n",
    "# Start with tender IDs\n",
    "ensemble = tenders[['tender_id', 'tender_value', 'procurement_method', 'year', \n",
    "                    'buyer_id', 'supplier_id', 'is_single_bidder', 'is_competitive']].copy()\n",
    "\n",
    "# Add Rule-based scores\n",
    "ensemble = ensemble.merge(\n",
    "    rule_results[['tender_id', 'rule_risk_score', 'rule_risk_level']],\n",
    "    on='tender_id', how='left'\n",
    ")\n",
    "\n",
    "# Add Statistical scores\n",
    "ensemble = ensemble.merge(\n",
    "    stat_results[['tender_id', 'stat_score']],\n",
    "    on='tender_id', how='left'\n",
    ")\n",
    "\n",
    "# Add IF scores\n",
    "ensemble = ensemble.merge(\n",
    "    if_results[['tender_id', 'if_score', 'if_anomaly']],\n",
    "    on='tender_id', how='left'\n",
    ")\n",
    "\n",
    "# Add LOF scores (only for sample)\n",
    "ensemble = ensemble.merge(\n",
    "    lof_results[['tender_id', 'lof_score', 'lof_anomaly']],\n",
    "    on='tender_id', how='left'\n",
    ")\n",
    "\n",
    "print(f\"Ensemble dataset: {len(ensemble):,} tenders\")\n",
    "print(f\"With LOF scores: {ensemble['lof_score'].notna().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Normalize Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all scores to 0-1 range\n",
    "print(\"Normalizing scores to 0-1 range...\")\n",
    "\n",
    "# Rule score: max is ~15-20, normalize\n",
    "rule_max = ensemble['rule_risk_score'].max()\n",
    "ensemble['rule_score_norm'] = ensemble['rule_risk_score'] / rule_max\n",
    "\n",
    "# Stat score: max is ~10-15\n",
    "stat_max = ensemble['stat_score'].max()\n",
    "ensemble['stat_score_norm'] = ensemble['stat_score'] / stat_max\n",
    "\n",
    "# IF score: already 0-1\n",
    "ensemble['if_score_norm'] = ensemble['if_score']\n",
    "\n",
    "# LOF score: already 0-1\n",
    "ensemble['lof_score_norm'] = ensemble['lof_score']\n",
    "\n",
    "print(f\"\\nScore ranges (normalized):\")\n",
    "print(f\"  Rule: 0 - {ensemble['rule_score_norm'].max():.2f}\")\n",
    "print(f\"  Stat: 0 - {ensemble['stat_score_norm'].max():.2f}\")\n",
    "print(f\"  IF:   0 - {ensemble['if_score_norm'].max():.2f}\")\n",
    "print(f\"  LOF:  0 - {ensemble['lof_score_norm'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Compute Ensemble Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble scoring methods\n",
    "\n",
    "# Method 1: Weighted average (without LOF for full dataset)\n",
    "WEIGHTS = {\n",
    "    'rule': 1.0,   # Process violations\n",
    "    'stat': 0.8,   # Statistical anomalies\n",
    "    'if': 1.0,     # Global outliers\n",
    "}\n",
    "\n",
    "ensemble['ensemble_score'] = (\n",
    "    ensemble['rule_score_norm'] * WEIGHTS['rule'] +\n",
    "    ensemble['stat_score_norm'] * WEIGHTS['stat'] +\n",
    "    ensemble['if_score_norm'] * WEIGHTS['if']\n",
    ") / sum(WEIGHTS.values())\n",
    "\n",
    "# Method 2: Consensus count (how many methods flagged)\n",
    "ensemble['rule_flag'] = (ensemble['rule_risk_score'] >= 6).astype(int)\n",
    "ensemble['stat_flag'] = (ensemble['stat_score'] >= 3).astype(int)\n",
    "ensemble['if_flag'] = ensemble['if_anomaly']\n",
    "\n",
    "ensemble['consensus_count'] = (\n",
    "    ensemble['rule_flag'] + \n",
    "    ensemble['stat_flag'] + \n",
    "    ensemble['if_flag']\n",
    ")\n",
    "\n",
    "# With LOF (for sample only)\n",
    "ensemble['consensus_with_lof'] = ensemble['consensus_count'] + ensemble['lof_anomaly'].fillna(0)\n",
    "\n",
    "print(\"Ensemble scores computed.\")\n",
    "print(f\"\\nWeights: {WEIGHTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign risk levels based on ensemble score\n",
    "def assign_risk_level(row):\n",
    "    if row['consensus_count'] >= 3:\n",
    "        return 'critical'\n",
    "    elif row['consensus_count'] == 2 or row['ensemble_score'] >= 0.7:\n",
    "        return 'high'\n",
    "    elif row['consensus_count'] == 1 or row['ensemble_score'] >= 0.4:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'low'\n",
    "\n",
    "ensemble['ensemble_risk'] = ensemble.apply(assign_risk_level, axis=1)\n",
    "\n",
    "# Distribution\n",
    "risk_dist = ensemble['ensemble_risk'].value_counts()\n",
    "print(\"\\nENSEMBLE RISK DISTRIBUTION:\")\n",
    "for level in ['critical', 'high', 'medium', 'low']:\n",
    "    count = risk_dist.get(level, 0)\n",
    "    pct = count / len(ensemble) * 100\n",
    "    print(f\"  {level:10} {count:>10,} ({pct:>5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Consensus Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consensus breakdown\n",
    "print(\"CONSENSUS BREAKDOWN (Rule + Stat + IF):\")\n",
    "consensus_dist = ensemble['consensus_count'].value_counts().sort_index()\n",
    "for count, num in consensus_dist.items():\n",
    "    pct = num / len(ensemble) * 100\n",
    "    methods = f\"{count}/3 methods\"\n",
    "    print(f\"  {methods}: {num:>10,} ({pct:>5.2f}%)\")\n",
    "\n",
    "# Critical: all 3 methods agree\n",
    "critical_consensus = ensemble[ensemble['consensus_count'] == 3]\n",
    "print(f\"\\nCRITICAL (all 3 methods): {len(critical_consensus):,} tenders\")\n",
    "print(f\"  Total value: {critical_consensus['tender_value'].sum() / 1e9:.2f} B UAH\")\n",
    "print(f\"  Mean value: {critical_consensus['tender_value'].mean() / 1e6:.2f} M UAH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize consensus\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Consensus distribution\n",
    "colors = ['#2ca02c', '#ffbb78', '#ff7f0e', '#d62728']\n",
    "axes[0].bar(consensus_dist.index, consensus_dist.values, color=colors)\n",
    "axes[0].set_xlabel('Number of Methods Flagged')\n",
    "axes[0].set_ylabel('Number of Tenders')\n",
    "axes[0].set_title('Consensus Distribution')\n",
    "axes[0].set_xticks([0, 1, 2, 3])\n",
    "axes[0].set_xticklabels(['0 (Normal)', '1 Method', '2 Methods', '3 Methods (Critical)'])\n",
    "\n",
    "# Risk level pie\n",
    "risk_colors = {'critical': '#d62728', 'high': '#ff7f0e', 'medium': '#ffbb78', 'low': '#2ca02c'}\n",
    "risk_order = ['critical', 'high', 'medium', 'low']\n",
    "risk_values = [risk_dist.get(r, 0) for r in risk_order]\n",
    "axes[1].pie(risk_values, labels=risk_order, colors=[risk_colors[r] for r in risk_order],\n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Ensemble Risk Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/ensemble/consensus_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. Method Contribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which methods contribute to critical cases?\n",
    "print(\"METHOD FLAGS IN CRITICAL TENDERS (consensus=3):\")\n",
    "print(f\"  Rule flagged: {critical_consensus['rule_flag'].sum():,} (100%)\")\n",
    "print(f\"  Stat flagged: {critical_consensus['stat_flag'].sum():,} (100%)\")\n",
    "print(f\"  IF flagged: {critical_consensus['if_flag'].sum():,} (100%)\")\n",
    "\n",
    "# Method overlap matrix\n",
    "print(\"\\nMETHOD OVERLAP:\")\n",
    "rule_only = ((ensemble['rule_flag'] == 1) & (ensemble['stat_flag'] == 0) & (ensemble['if_flag'] == 0)).sum()\n",
    "stat_only = ((ensemble['rule_flag'] == 0) & (ensemble['stat_flag'] == 1) & (ensemble['if_flag'] == 0)).sum()\n",
    "if_only = ((ensemble['rule_flag'] == 0) & (ensemble['stat_flag'] == 0) & (ensemble['if_flag'] == 1)).sum()\n",
    "\n",
    "print(f\"  Only Rule: {rule_only:,}\")\n",
    "print(f\"  Only Stat: {stat_only:,}\")\n",
    "print(f\"  Only IF: {if_only:,}\")\n",
    "\n",
    "rule_stat = ((ensemble['rule_flag'] == 1) & (ensemble['stat_flag'] == 1) & (ensemble['if_flag'] == 0)).sum()\n",
    "rule_if = ((ensemble['rule_flag'] == 1) & (ensemble['stat_flag'] == 0) & (ensemble['if_flag'] == 1)).sum()\n",
    "stat_if = ((ensemble['rule_flag'] == 0) & (ensemble['stat_flag'] == 1) & (ensemble['if_flag'] == 1)).sum()\n",
    "\n",
    "print(f\"  Rule + Stat: {rule_stat:,}\")\n",
    "print(f\"  Rule + IF: {rule_if:,}\")\n",
    "print(f\"  Stat + IF: {stat_if:,}\")\n",
    "print(f\"  All three: {len(critical_consensus):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Venn-style bar chart\n",
    "overlap_data = {\n",
    "    'Only Rule': rule_only,\n",
    "    'Only Stat': stat_only,\n",
    "    'Only IF': if_only,\n",
    "    'Rule+Stat': rule_stat,\n",
    "    'Rule+IF': rule_if,\n",
    "    'Stat+IF': stat_if,\n",
    "    'All Three': len(critical_consensus),\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "colors = ['#1f77b4', '#2ca02c', '#ff7f0e', '#9467bd', '#8c564b', '#e377c2', '#d62728']\n",
    "bars = ax.bar(overlap_data.keys(), overlap_data.values(), color=colors)\n",
    "ax.set_ylabel('Number of Tenders')\n",
    "ax.set_title('Method Overlap Analysis')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height):,}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/ensemble/method_overlap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 8. Critical Tenders Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characteristics of critical tenders\n",
    "print(\"=\"*60)\n",
    "print(\"CRITICAL TENDERS CHARACTERISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "normal = ensemble[ensemble['consensus_count'] == 0]\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'Critical':>15} {'Normal':>15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Value\n",
    "print(f\"{'Mean tender value (M UAH)':<30} {critical_consensus['tender_value'].mean()/1e6:>15,.2f} {normal['tender_value'].mean()/1e6:>15,.2f}\")\n",
    "print(f\"{'Median tender value (K UAH)':<30} {critical_consensus['tender_value'].median()/1e3:>15,.1f} {normal['tender_value'].median()/1e3:>15,.1f}\")\n",
    "\n",
    "# Competition\n",
    "print(f\"{'Single bidder rate (%)':<30} {critical_consensus['is_single_bidder'].mean()*100:>15.1f} {normal['is_single_bidder'].mean()*100:>15.1f}\")\n",
    "print(f\"{'Competitive rate (%)':<30} {critical_consensus['is_competitive'].mean()*100:>15.1f} {normal['is_competitive'].mean()*100:>15.1f}\")\n",
    "\n",
    "# Scores\n",
    "print(f\"{'Mean rule score':<30} {critical_consensus['rule_risk_score'].mean():>15.1f} {normal['rule_risk_score'].mean():>15.1f}\")\n",
    "print(f\"{'Mean stat score':<30} {critical_consensus['stat_score'].mean():>15.1f} {normal['stat_score'].mean():>15.1f}\")\n",
    "print(f\"{'Mean IF score':<30} {critical_consensus['if_score'].mean():>15.3f} {normal['if_score'].mean():>15.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procurement method distribution\n",
    "print(\"\\nPROCUREMENT METHOD:\")\n",
    "critical_method = critical_consensus['procurement_method'].value_counts(normalize=True) * 100\n",
    "normal_method = normal['procurement_method'].value_counts(normalize=True) * 100\n",
    "\n",
    "for method in critical_method.index:\n",
    "    c_pct = critical_method.get(method, 0)\n",
    "    n_pct = normal_method.get(method, 0)\n",
    "    ratio = c_pct / n_pct if n_pct > 0 else 0\n",
    "    print(f\"  {method}: {c_pct:.1f}% (vs {n_pct:.1f}% normal) - {ratio:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year distribution\n",
    "print(\"\\nYEAR DISTRIBUTION:\")\n",
    "critical_year = critical_consensus['year'].value_counts().sort_index()\n",
    "for year, count in critical_year.items():\n",
    "    total_year = len(ensemble[ensemble['year'] == year])\n",
    "    pct = count / total_year * 100\n",
    "    print(f\"  {year}: {count:,} ({pct:.2f}% of year)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 9. Top Risky Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top buyers by critical tender count\n",
    "print(\"TOP 10 BUYERS BY CRITICAL TENDERS:\")\n",
    "top_buyers_critical = critical_consensus.groupby('buyer_id').agg({\n",
    "    'tender_id': 'count',\n",
    "    'tender_value': 'sum',\n",
    "    'ensemble_score': 'mean'\n",
    "}).sort_values('tender_id', ascending=False).head(10)\n",
    "\n",
    "top_buyers_critical = top_buyers_critical.reset_index().merge(\n",
    "    buyers[['buyer_id', 'buyer_name', 'buyer_region']], on='buyer_id', how='left'\n",
    ")\n",
    "\n",
    "for _, row in top_buyers_critical.iterrows():\n",
    "    name = str(row['buyer_name'])[:50] if pd.notna(row['buyer_name']) else 'N/A'\n",
    "    print(f\"  {row['tender_id']:>5,} tenders | {row['tender_value']/1e6:>10,.1f}M UAH | {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top suppliers by critical tender count\n",
    "print(\"\\nTOP 10 SUPPLIERS BY CRITICAL TENDERS:\")\n",
    "top_suppliers_critical = critical_consensus.groupby('supplier_id').agg({\n",
    "    'tender_id': 'count',\n",
    "    'tender_value': 'sum',\n",
    "    'ensemble_score': 'mean'\n",
    "}).sort_values('tender_id', ascending=False).head(10)\n",
    "\n",
    "top_suppliers_critical = top_suppliers_critical.reset_index().merge(\n",
    "    suppliers[['supplier_id', 'supplier_name']], on='supplier_id', how='left'\n",
    ")\n",
    "\n",
    "for _, row in top_suppliers_critical.iterrows():\n",
    "    name = str(row['supplier_name'])[:50] if pd.notna(row['supplier_name']) else 'N/A'\n",
    "    print(f\"  {row['tender_id']:>5,} tenders | {row['tender_value']/1e6:>10,.1f}M UAH | {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ensemble results\n",
    "output_cols = [\n",
    "    'tender_id', 'tender_value', 'procurement_method', 'year',\n",
    "    'buyer_id', 'supplier_id',\n",
    "    'rule_risk_score', 'stat_score', 'if_score', 'lof_score',\n",
    "    'ensemble_score', 'consensus_count', 'ensemble_risk'\n",
    "]\n",
    "\n",
    "# Save full results\n",
    "ensemble[output_cols].to_csv('../results/ensemble_results.csv', index=False)\n",
    "print(f\"Saved full results: results/ensemble_results.csv ({len(ensemble):,} rows)\")\n",
    "\n",
    "# Save critical only\n",
    "critical_consensus[output_cols].to_csv('../results/critical_tenders.csv', index=False)\n",
    "print(f\"Saved critical tenders: results/critical_tenders.csv ({len(critical_consensus):,} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ENSEMBLE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDataset: {len(ensemble):,} tenders ({YEARS[0]}-{YEARS[-1]})\")\n",
    "print(f\"\\nMethods combined:\")\n",
    "print(f\"  1. Rule-based: {ensemble['rule_flag'].sum():,} high-risk\")\n",
    "print(f\"  2. Statistical: {ensemble['stat_flag'].sum():,} high-risk\")\n",
    "print(f\"  3. Isolation Forest: {ensemble['if_flag'].sum():,} anomalies\")\n",
    "print(f\"  4. LOF (sample): {lof_results['lof_anomaly'].sum():,} anomalies\")\n",
    "\n",
    "print(f\"\\nENSEMBLE RISK LEVELS:\")\n",
    "for level in ['critical', 'high', 'medium', 'low']:\n",
    "    count = risk_dist.get(level, 0)\n",
    "    pct = count / len(ensemble) * 100\n",
    "    value = ensemble[ensemble['ensemble_risk'] == level]['tender_value'].sum() / 1e9\n",
    "    print(f\"  {level:10} {count:>10,} ({pct:>5.2f}%) - {value:>8.2f}B UAH\")\n",
    "\n",
    "print(f\"\\nCRITICAL TENDERS (all 3 methods agree):\")\n",
    "print(f\"  Count: {len(critical_consensus):,}\")\n",
    "print(f\"  Total value: {critical_consensus['tender_value'].sum()/1e9:.2f}B UAH\")\n",
    "print(f\"  Mean value: {critical_consensus['tender_value'].mean()/1e6:.2f}M UAH\")\n",
    "print(f\"  Single bidder rate: {critical_consensus['is_single_bidder'].mean()*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## Висновки\n",
    "\n",
    "### Ensemble підхід:\n",
    "- **Consensus voting**: кількість методів, що flagged тендер\n",
    "- **Weighted score**: зважена комбінація нормалізованих scores\n",
    "- **Risk levels**: critical (3/3), high (2/3), medium (1/3), low (0/3)\n",
    "\n",
    "### Ключові результати:\n",
    "- **Critical** (всі 3 методи згодні) — найвища впевненість для аудиту\n",
    "- Методи доповнюють один одного (низька кореляція)\n",
    "- Різні типи аномалій покриваються різними методами\n",
    "\n",
    "### Збережені файли:\n",
    "- `results/ensemble_results.csv` — всі тендери з scores\n",
    "- `results/critical_tenders.csv` — лише critical (consensus=3)\n",
    "\n",
    "### Наступні кроки:\n",
    "- Детальний аналіз top buyers/suppliers\n",
    "- DBSCAN для виявлення картелів\n",
    "- Звіт для thesis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
