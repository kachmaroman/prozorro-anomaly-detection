{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# HDBSCAN: Clustering + Outlier Detection\n",
    "\n",
    "**Мета:** Замінити LOF + DBSCAN одним алгоритмом HDBSCAN.\n",
    "\n",
    "**HDBSCAN (Hierarchical DBSCAN):**\n",
    "- Кластеризація без параметра `eps` (на відміну від DBSCAN)\n",
    "- Outlier scores (як LOF)\n",
    "- Швидший і масштабованіший за LOF\n",
    "- Знаходить кластери різної щільності\n",
    "\n",
    "**Що отримуємо:**\n",
    "1. `labels_` — кластери (-1 = outlier) → для виявлення картелів\n",
    "2. `outlier_scores_` — score 0-1 → замість LOF для ensemble\n",
    "3. `probabilities_` — впевненість приналежності до кластера\n",
    "\n",
    "**Pipeline:**\n",
    "1. Rule-based ✓\n",
    "2. Statistical ✓\n",
    "3. Isolation Forest ✓\n",
    "4. **HDBSCAN** ← current (замінює LOF + DBSCAN)\n",
    "5. Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '..')\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.impute import SimpleImputer\n\nfrom src.data_loader import load_tenders, load_buyers, load_suppliers, memory_usage\nfrom src.detectors import HDBSCANDetector, PyODDetector\n\n# ============================================================\n# CONFIGURATION\n# ============================================================\nYEARS = [2022, 2023, 2024, 2025]\nSAMPLE_SIZE = 1_000_000  # HDBSCAN на sample (швидше за LOF, але все ще потребує RAM)\nMIN_CLUSTER_SIZE = 50    # Мінімальний розмір кластера\nMIN_SAMPLES = 10         # Мінімум точок для core point\nCONTAMINATION = 0.05     # Expected anomaly rate\n# ============================================================\n\n# Create output directories\nPath('../results/figures/hdbscan').mkdir(parents=True, exist_ok=True)\n\n# Style\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['font.size'] = 11\n\nprint(f\"Configuration:\")\nprint(f\"  YEARS = {YEARS}\")\nprint(f\"  SAMPLE_SIZE = {SAMPLE_SIZE:,}\")\nprint(f\"  MIN_CLUSTER_SIZE = {MIN_CLUSTER_SIZE}\")\nprint(f\"  MIN_SAMPLES = {MIN_SAMPLES}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Завантаження даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "tenders = load_tenders(years=YEARS)\n",
    "buyers = load_buyers()\n",
    "suppliers = load_suppliers()\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Tenders: {len(tenders):,}\")\n",
    "print(f\"  Buyers: {len(buyers):,}\")\n",
    "print(f\"  Suppliers: {len(suppliers):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended features (same as IF)\n",
    "print(\"Computing extended features...\")\n",
    "\n",
    "# Benchmark features\n",
    "cpv_stats = tenders.groupby('main_cpv_2_digit').agg({\n",
    "    'tender_value': 'median',\n",
    "    'award_value': 'median',\n",
    "    'number_of_tenderers': 'mean',\n",
    "}).rename(columns={\n",
    "    'tender_value': 'cpv_median_value',\n",
    "    'award_value': 'cpv_median_award',\n",
    "    'number_of_tenderers': 'cpv_avg_tenderers',\n",
    "})\n",
    "\n",
    "tenders = tenders.merge(cpv_stats, on='main_cpv_2_digit', how='left')\n",
    "tenders['price_vs_cpv_median'] = tenders['award_value'] / tenders['cpv_median_award'].replace(0, np.nan)\n",
    "tenders['tenderers_vs_cpv_avg'] = tenders['number_of_tenderers'] / tenders['cpv_avg_tenderers'].replace(0, np.nan)\n",
    "\n",
    "# Pair features\n",
    "pair_stats = tenders.groupby(['buyer_id', 'supplier_id']).agg({\n",
    "    'tender_id': 'count',\n",
    "    'award_value': 'sum'\n",
    "}).rename(columns={'tender_id': 'pair_contract_count', 'award_value': 'pair_total_value'})\n",
    "\n",
    "buyer_total = tenders.groupby('buyer_id')['award_value'].sum()\n",
    "pair_stats = pair_stats.reset_index()\n",
    "pair_stats = pair_stats.merge(buyer_total.rename('buyer_total_value'), on='buyer_id')\n",
    "pair_stats['pair_share_of_buyer'] = pair_stats['pair_total_value'] / pair_stats['buyer_total_value'].replace(0, np.nan)\n",
    "\n",
    "tenders = tenders.merge(\n",
    "    pair_stats[['buyer_id', 'supplier_id', 'pair_contract_count', 'pair_share_of_buyer']],\n",
    "    on=['buyer_id', 'supplier_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Log transform\n",
    "tenders['log_tender_value'] = np.log1p(tenders['tender_value'])\n",
    "\n",
    "print(\"Feature engineering complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "print(f\"\\nSampling {SAMPLE_SIZE:,} tenders...\")\n",
    "tenders_sample = tenders.sample(SAMPLE_SIZE, random_state=42)\n",
    "print(f\"Sample: {len(tenders_sample):,} ({len(tenders_sample)/len(tenders)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature configuration\n",
    "features_extended = {\n",
    "    \"tender\": [\n",
    "        \"tender_value\", \"price_change_pct\", \"number_of_tenderers\",\n",
    "        \"is_single_bidder\", \"is_competitive\", \"log_tender_value\",\n",
    "        \"price_vs_cpv_median\", \"tenderers_vs_cpv_avg\",\n",
    "        \"pair_contract_count\", \"pair_share_of_buyer\",\n",
    "        \"is_weekend\", \"is_q4\", \"is_december\",\n",
    "    ],\n",
    "    \"buyer\": [\n",
    "        \"single_bidder_rate\", \"competitive_rate\",\n",
    "        \"avg_discount_pct\", \"supplier_diversity_index\",\n",
    "    ],\n",
    "    \"supplier\": [\"total_awards\", \"total_value\"],\n",
    "}\n",
    "\n",
    "print(f\"Features: {sum(len(v) for v in features_extended.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrix\n",
    "def prepare_features(df, buyers_df, suppliers_df, features_config):\n",
    "    \"\"\"Prepare feature matrix for HDBSCAN.\"\"\"\n",
    "    X = df.copy()\n",
    "    feature_cols = []\n",
    "    \n",
    "    # Tender features\n",
    "    for col in features_config.get('tender', []):\n",
    "        if col in X.columns:\n",
    "            feature_cols.append(col)\n",
    "    \n",
    "    # Merge buyer features\n",
    "    if buyers_df is not None:\n",
    "        buyer_cols = [c for c in features_config.get('buyer', []) if c in buyers_df.columns]\n",
    "        if buyer_cols:\n",
    "            X = X.merge(buyers_df[['buyer_id'] + buyer_cols], on='buyer_id', how='left')\n",
    "            feature_cols.extend(buyer_cols)\n",
    "    \n",
    "    # Merge supplier features\n",
    "    if suppliers_df is not None:\n",
    "        supplier_cols = [c for c in features_config.get('supplier', []) if c in suppliers_df.columns]\n",
    "        if supplier_cols:\n",
    "            X = X.merge(suppliers_df[['supplier_id'] + supplier_cols], on='supplier_id', how='left')\n",
    "            feature_cols.extend(supplier_cols)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    feature_cols = list(dict.fromkeys(feature_cols))\n",
    "    \n",
    "    return X, X[feature_cols], feature_cols\n",
    "\n",
    "X_full, X_features, feature_names = prepare_features(\n",
    "    tenders_sample, buyers, suppliers, features_extended\n",
    ")\n",
    "\n",
    "print(f\"Feature matrix: {X_features.shape}\")\n",
    "print(f\"Features: {feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess: impute + scale\n",
    "print(\"Preprocessing...\")\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X_features)\n",
    "\n",
    "# Scale\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "print(f\"Preprocessed shape: {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 3. HDBSCAN Clustering + Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run HDBSCAN using detector class\n",
    "print(f\"Running HDBSCAN (min_cluster_size={MIN_CLUSTER_SIZE}, min_samples={MIN_SAMPLES})...\")\n",
    "\n",
    "# Extended features for HDBSCAN\n",
    "features_extended = {\n",
    "    \"tender\": [\n",
    "        \"tender_value\", \"price_change_pct\", \"number_of_tenderers\",\n",
    "        \"is_single_bidder\", \"is_competitive\", \"log_tender_value\",\n",
    "        \"price_vs_cpv_median\", \"tenderers_vs_cpv_avg\",\n",
    "        \"pair_contract_count\", \"pair_share_of_buyer\",\n",
    "        \"is_weekend\", \"is_q4\", \"is_december\",\n",
    "    ],\n",
    "    \"buyer\": [\n",
    "        \"single_bidder_rate\", \"competitive_rate\",\n",
    "        \"avg_discount_pct\", \"supplier_diversity_index\",\n",
    "    ],\n",
    "    \"supplier\": [\"total_awards\", \"total_value\"],\n",
    "}\n",
    "\n",
    "# Initialize detector\n",
    "hdbscan_detector = HDBSCANDetector(\n",
    "    min_cluster_size=MIN_CLUSTER_SIZE,\n",
    "    min_samples=MIN_SAMPLES,\n",
    "    contamination=CONTAMINATION,\n",
    "    features=features_extended,\n",
    ")\n",
    "\n",
    "# Run detection (with sampling)\n",
    "hdbscan_results = hdbscan_detector.fit_detect(\n",
    "    tenders,\n",
    "    buyers_df=buyers,\n",
    "    suppliers_df=suppliers,\n",
    "    sample_size=SAMPLE_SIZE,\n",
    ")\n",
    "\n",
    "print(\"\\nHDBSCAN complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results summary using detector\n",
    "print(\"HDBSCAN RESULTS:\")\n",
    "print(hdbscan_detector.summary().to_string(index=False))\n",
    "\n",
    "# Extract key variables for later analysis\n",
    "labels = hdbscan_results['hdbscan_cluster'].values\n",
    "probabilities = hdbscan_results['hdbscan_probability'].values\n",
    "outlier_scores = hdbscan_results['hdbscan_score'].values\n",
    "\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = (labels == -1).sum()\n",
    "\n",
    "print(f\"\\nOutlier scores (1 - probability):\")\n",
    "print(f\"  Min: {outlier_scores.min():.4f}\")\n",
    "print(f\"  Max: {outlier_scores.max():.4f}\")\n",
    "print(f\"  Mean: {outlier_scores.mean():.4f}\")\n",
    "print(f\"  Noise points have score = 1.0 (probability = 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection summary\n",
    "# The detector already computes anomaly flags based on contamination rate\n",
    "\n",
    "threshold_95 = np.percentile(outlier_scores, 95)\n",
    "print(f\"Outlier score threshold (95th percentile): {threshold_95:.4f}\")\n",
    "print(f\"Anomalies: {hdbscan_results['hdbscan_anomaly'].sum():,} ({hdbscan_results['hdbscan_anomaly'].mean()*100:.2f}%)\")\n",
    "\n",
    "# Risk level distribution\n",
    "risk_dist = hdbscan_results['hdbscan_risk_level'].value_counts()\n",
    "print(f\"\\nRisk level distribution:\")\n",
    "for level in ['critical', 'high', 'medium', 'low']:\n",
    "    count = risk_dist.get(level, 0)\n",
    "    print(f\"  {level}: {count:,} ({count/len(hdbscan_results)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 4. Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster size distribution\n",
    "cluster_sizes = pd.Series(labels[labels != -1]).value_counts().sort_values(ascending=False)\n",
    "\n",
    "print(\"CLUSTER SIZE DISTRIBUTION:\")\n",
    "print(f\"  Largest cluster: {cluster_sizes.iloc[0]:,} points\")\n",
    "print(f\"  Smallest cluster: {cluster_sizes.iloc[-1]:,} points\")\n",
    "print(f\"  Median cluster size: {cluster_sizes.median():.0f} points\")\n",
    "\n",
    "print(f\"\\nTop 10 clusters:\")\n",
    "for i, (cluster_id, size) in enumerate(cluster_sizes.head(10).items()):\n",
    "    print(f\"  Cluster {cluster_id}: {size:,} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cluster sizes (top 20)\n",
    "top_clusters = cluster_sizes.head(20)\n",
    "axes[0].bar(range(len(top_clusters)), top_clusters.values, color='steelblue')\n",
    "axes[0].set_xlabel('Cluster Rank')\n",
    "axes[0].set_ylabel('Number of Points')\n",
    "axes[0].set_title('Top 20 Clusters by Size')\n",
    "\n",
    "# Outlier score distribution (1 - probability)\n",
    "threshold_95 = np.percentile(outlier_scores, 95)\n",
    "axes[1].hist(outlier_scores, bins=50, color='coral', edgecolor='white')\n",
    "axes[1].axvline(threshold_95, color='red', linestyle='--', label=f'95th percentile ({threshold_95:.3f})')\n",
    "axes[1].set_xlabel('Outlier Score (1 - probability)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('HDBSCAN Outlier Score Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/hdbscan/cluster_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze clusters - what characterizes each cluster?\n",
    "analysis_df = tenders_sample.merge(hdbscan_results, on='tender_id')\n",
    "\n",
    "# Cluster characteristics\n",
    "cluster_stats = analysis_df[analysis_df['hdbscan_cluster'] != -1].groupby('hdbscan_cluster').agg({\n",
    "    'tender_id': 'count',\n",
    "    'tender_value': ['mean', 'median'],\n",
    "    'is_single_bidder': 'mean',\n",
    "    'is_competitive': 'mean',\n",
    "    'price_change_pct': 'mean',\n",
    "}).round(2)\n",
    "\n",
    "cluster_stats.columns = ['count', 'mean_value', 'median_value', 'single_bidder_rate', 'competitive_rate', 'avg_discount']\n",
    "cluster_stats = cluster_stats.sort_values('count', ascending=False)\n",
    "\n",
    "print(\"TOP 10 CLUSTER CHARACTERISTICS:\")\n",
    "print(cluster_stats.head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 5. Comparison with Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "# Run IF on same sample for comparison\nprint(\"Running Isolation Forest for comparison...\")\n\n# Get the sampled tender_ids from HDBSCAN results\nsampled_tender_ids = hdbscan_results['tender_id'].unique()\ntenders_sample = tenders[tenders['tender_id'].isin(sampled_tender_ids)].copy()\n\nif_detector = PyODDetector(\n    algorithm=\"iforest\",\n    contamination=CONTAMINATION,\n    random_state=42,\n    features=features_extended,\n)\n\nif_results = if_detector.fit_detect(tenders_sample, buyers_df=buyers, suppliers_df=suppliers)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "# Merge results for comparison\n# PyODDetector uses 'score' and 'anomaly' column names\ncomparison = hdbscan_results.merge(if_results[['tender_id', 'score', 'anomaly']], on='tender_id')\ncomparison = comparison.rename(columns={'score': 'if_score', 'anomaly': 'if_anomaly'})\n\n# Agreement\nboth = ((comparison['hdbscan_anomaly'] == 1) & (comparison['if_anomaly'] == 1)).sum()\nonly_hdbscan = ((comparison['hdbscan_anomaly'] == 1) & (comparison['if_anomaly'] == 0)).sum()\nonly_if = ((comparison['hdbscan_anomaly'] == 0) & (comparison['if_anomaly'] == 1)).sum()\nneither = ((comparison['hdbscan_anomaly'] == 0) & (comparison['if_anomaly'] == 0)).sum()\n\nprint(\"=\"*60)\nprint(\"HDBSCAN vs ISOLATION FOREST\")\nprint(\"=\"*60)\nprint(f\"\\nAGREEMENT MATRIX:\")\nprint(f\"                    IF Normal     IF Anomaly\")\nprint(f\"HDBSCAN Normal      {neither:>10,}    {only_if:>10,}\")\nprint(f\"HDBSCAN Anomaly     {only_hdbscan:>10,}    {both:>10,}\")\n\n# Metrics\nscore_corr = comparison['hdbscan_score'].corr(comparison['if_score'])\nhdbscan_set = set(comparison[comparison['hdbscan_anomaly'] == 1]['tender_id'])\nif_set = set(comparison[comparison['if_anomaly'] == 1]['tender_id'])\njaccard = len(hdbscan_set & if_set) / len(hdbscan_set | if_set) if len(hdbscan_set | if_set) > 0 else 0\n\nprint(f\"\\nMETRICS:\")\nprint(f\"  Score correlation: {score_corr:.3f}\")\nprint(f\"  Jaccard similarity: {jaccard:.3f}\")\nprint(f\"  Both flagged: {both:,} ({both/len(comparison)*100:.2f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Score scatter\n",
    "sample_viz = comparison.sample(min(10000, len(comparison)), random_state=42)\n",
    "axes[0].scatter(sample_viz['if_score'], sample_viz['hdbscan_score'], alpha=0.3, s=5)\n",
    "axes[0].set_xlabel('Isolation Forest Score')\n",
    "axes[0].set_ylabel('HDBSCAN Outlier Score')\n",
    "axes[0].set_title(f'Score Correlation: {score_corr:.3f}')\n",
    "\n",
    "# Agreement pie\n",
    "labels_pie = ['Both', 'Only HDBSCAN', 'Only IF', 'Neither']\n",
    "sizes = [both, only_hdbscan, only_if, neither]\n",
    "colors = ['#d62728', '#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "axes[1].pie(sizes, labels=labels_pie, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('HDBSCAN vs IF Agreement')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/hdbscan/if_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 6. Noise Points Analysis (Potential Fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze noise points (outliers)\n",
    "noise_df = analysis_df[analysis_df['hdbscan_cluster'] == -1]\n",
    "clustered_df = analysis_df[analysis_df['hdbscan_cluster'] != -1]\n",
    "\n",
    "print(\"NOISE vs CLUSTERED COMPARISON:\")\n",
    "print(f\"\\n{'Metric':<30} {'Noise':>15} {'Clustered':>15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Count':<30} {len(noise_df):>15,} {len(clustered_df):>15,}\")\n",
    "print(f\"{'Mean tender value (M UAH)':<30} {noise_df['tender_value'].mean()/1e6:>15.2f} {clustered_df['tender_value'].mean()/1e6:>15.2f}\")\n",
    "print(f\"{'Median tender value (K UAH)':<30} {noise_df['tender_value'].median()/1e3:>15.1f} {clustered_df['tender_value'].median()/1e3:>15.1f}\")\n",
    "print(f\"{'Single bidder rate (%)':<30} {noise_df['is_single_bidder'].mean()*100:>15.1f} {clustered_df['is_single_bidder'].mean()*100:>15.1f}\")\n",
    "print(f\"{'Competitive rate (%)':<30} {noise_df['is_competitive'].mean()*100:>15.1f} {clustered_df['is_competitive'].mean()*100:>15.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procurement method distribution\n",
    "print(\"\\nPROCUREMENT METHOD:\")\n",
    "noise_method = noise_df['procurement_method'].value_counts(normalize=True) * 100\n",
    "clustered_method = clustered_df['procurement_method'].value_counts(normalize=True) * 100\n",
    "\n",
    "for method in noise_method.index:\n",
    "    n_pct = noise_method.get(method, 0)\n",
    "    c_pct = clustered_method.get(method, 0)\n",
    "    print(f\"  {method}: Noise {n_pct:.1f}% vs Clustered {c_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 7. Suspicious Clusters (Potential Cartels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find clusters with suspicious characteristics using detector method\n",
    "suspicious_clusters = hdbscan_detector.get_suspicious_clusters(\n",
    "    min_size=100,\n",
    "    min_single_bidder_rate=0.5\n",
    ")\n",
    "\n",
    "print(f\"SUSPICIOUS CLUSTERS (single_bidder > 50%, count >= 100):\")\n",
    "print(f\"Found: {len(suspicious_clusters)} clusters\")\n",
    "\n",
    "if len(suspicious_clusters) > 0:\n",
    "    print(suspicious_clusters.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze suppliers in suspicious clusters\n",
    "if len(suspicious_clusters) > 0:\n",
    "    suspicious_cluster_ids = suspicious_clusters.index.tolist()[:5]  # Top 5\n",
    "    \n",
    "    print(\"\\nTOP SUPPLIERS IN SUSPICIOUS CLUSTERS:\")\n",
    "    for cluster_id in suspicious_cluster_ids:\n",
    "        cluster_data = analysis_df[analysis_df['hdbscan_cluster'] == cluster_id]\n",
    "        top_suppliers = cluster_data['supplier_id'].value_counts().head(5)\n",
    "        \n",
    "        print(f\"\\nCluster {cluster_id} ({len(cluster_data)} tenders):\")\n",
    "        for supplier_id, count in top_suppliers.items():\n",
    "            supplier_name = suppliers[suppliers['supplier_id'] == supplier_id]['supplier_name'].values\n",
    "            name = supplier_name[0][:50] if len(supplier_name) > 0 else 'N/A'\n",
    "            print(f\"  {count:>5} tenders: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dwti48ihgq",
   "metadata": {},
   "source": [
    "## 8. Cartel Detection: Network Analysis\n",
    "\n",
    "Картель - це група постачальників, які:\n",
    "1. **Часто беруть участь в одних тендерах** (co-bidding)\n",
    "2. **Виграють по черзі** (bid rotation)\n",
    "3. **Мають схожі ціни** (price coordination)\n",
    "\n",
    "**Метод:**\n",
    "1. Завантажити bids → побачити всіх учасників тендеру\n",
    "2. Побудувати co-bidding network (граф)\n",
    "3. Знайти connected components → потенційні картелі\n",
    "4. Перевірити bid rotation pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bids for cartel analysis\n",
    "from src.data_loader import load_bids\n",
    "\n",
    "print(\"Loading bids data...\")\n",
    "bids = load_bids(years=YEARS)\n",
    "print(f\"Bids: {len(bids):,}\")\n",
    "\n",
    "# Filter to competitive tenders only (where cartels operate)\n",
    "competitive_tenders = tenders_sample[tenders_sample['number_of_tenderers'] >= 2]['tender_id'].unique()\n",
    "bids_competitive = bids[bids['tender_id'].isin(competitive_tenders)]\n",
    "print(f\"Bids in competitive tenders (2+ bidders): {len(bids_competitive):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zneubk36y8p",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build co-bidding network\n",
    "# Two suppliers are connected if they bid on the same tender\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"Building co-bidding network...\")\n",
    "\n",
    "# Filter out NA bidder_ids\n",
    "bids_clean = bids_competitive[bids_competitive['bidder_id'].notna()].copy()\n",
    "print(f\"Bids after removing NA bidder_id: {len(bids_clean):,}\")\n",
    "\n",
    "# Get all bidder pairs per tender\n",
    "co_bids = defaultdict(int)\n",
    "tender_bidders = bids_clean.groupby('tender_id')['bidder_id'].apply(list)\n",
    "\n",
    "for tender_id, bidders in tender_bidders.items():\n",
    "    # Filter out any remaining NA and get unique bidders\n",
    "    valid_bidders = [b for b in bidders if pd.notna(b)]\n",
    "    if len(valid_bidders) >= 2:\n",
    "        # All pairs of bidders\n",
    "        for pair in combinations(sorted(set(valid_bidders)), 2):\n",
    "            co_bids[pair] += 1\n",
    "\n",
    "print(f\"Unique bidder pairs: {len(co_bids):,}\")\n",
    "\n",
    "# Convert to dataframe\n",
    "co_bid_df = pd.DataFrame([\n",
    "    {'bidder_1': k[0], 'bidder_2': k[1], 'co_bid_count': v}\n",
    "    for k, v in co_bids.items()\n",
    "])\n",
    "\n",
    "# Filter to pairs that co-bid frequently (potential cartels)\n",
    "MIN_CO_BIDS = 5  # At least 5 tenders together\n",
    "frequent_pairs = co_bid_df[co_bid_df['co_bid_count'] >= MIN_CO_BIDS].sort_values('co_bid_count', ascending=False)\n",
    "print(f\"Pairs with {MIN_CO_BIDS}+ co-bids: {len(frequent_pairs):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962gpy1dlg5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top co-bidding pairs\n",
    "print(\"TOP 20 CO-BIDDING PAIRS (potential cartels):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Note: bidder_id often equals supplier_id for winners\n",
    "# Use suppliers table for names (bidders table doesn't have names)\n",
    "for i, row in frequent_pairs.head(20).iterrows():\n",
    "    # Try to find name in suppliers (bidder_id = supplier_id for winners)\n",
    "    b1_name = suppliers[suppliers['supplier_id'] == row['bidder_1']]['supplier_name'].values\n",
    "    b2_name = suppliers[suppliers['supplier_id'] == row['bidder_2']]['supplier_name'].values\n",
    "    \n",
    "    name1 = b1_name[0][:40] if len(b1_name) > 0 else f\"Bidder {row['bidder_1']}\"\n",
    "    name2 = b2_name[0][:40] if len(b2_name) > 0 else f\"Bidder {row['bidder_2']}\"\n",
    "    \n",
    "    print(f\"\\n{row['co_bid_count']:>3} tenders together:\")\n",
    "    print(f\"    1: {name1}\")\n",
    "    print(f\"    2: {name2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u6vtq4lqg2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bid Rotation Analysis\n",
    "# For each co-bidding pair, check who wins when they compete\n",
    "print(\"\\nBID ROTATION ANALYSIS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def analyze_pair_wins(bidder_1, bidder_2, bids_df, tenders_df):\n",
    "    \"\"\"Analyze win patterns for a pair of bidders.\"\"\"\n",
    "    # Find tenders where both bid\n",
    "    b1_tenders = set(bids_df[bids_df['bidder_id'] == bidder_1]['tender_id'])\n",
    "    b2_tenders = set(bids_df[bids_df['bidder_id'] == bidder_2]['tender_id'])\n",
    "    common_tenders = b1_tenders & b2_tenders\n",
    "    \n",
    "    if not common_tenders:\n",
    "        return None\n",
    "    \n",
    "    # Get winners for these tenders\n",
    "    common_df = tenders_df[tenders_df['tender_id'].isin(common_tenders)]\n",
    "    \n",
    "    # Count wins (supplier_id is the winner)\n",
    "    # We need to match bidder_id to supplier_id\n",
    "    b1_wins = common_df[common_df['supplier_id'] == bidder_1].shape[0]\n",
    "    b2_wins = common_df[common_df['supplier_id'] == bidder_2].shape[0]\n",
    "    other_wins = len(common_tenders) - b1_wins - b2_wins\n",
    "    \n",
    "    return {\n",
    "        'common_tenders': len(common_tenders),\n",
    "        'b1_wins': b1_wins,\n",
    "        'b2_wins': b2_wins,\n",
    "        'other_wins': other_wins,\n",
    "        'rotation_score': min(b1_wins, b2_wins) / max(b1_wins, b2_wins) if max(b1_wins, b2_wins) > 0 else 0\n",
    "    }\n",
    "\n",
    "# Analyze top pairs\n",
    "suspicious_pairs = []\n",
    "for i, row in frequent_pairs.head(50).iterrows():\n",
    "    result = analyze_pair_wins(row['bidder_1'], row['bidder_2'], bids_competitive, tenders_sample)\n",
    "    if result and result['b1_wins'] > 0 and result['b2_wins'] > 0:\n",
    "        suspicious_pairs.append({\n",
    "            **row.to_dict(),\n",
    "            **result\n",
    "        })\n",
    "\n",
    "suspicious_df = pd.DataFrame(suspicious_pairs)\n",
    "if len(suspicious_df) > 0:\n",
    "    # High rotation score = both win roughly equally often (suspicious!)\n",
    "    suspicious_df = suspicious_df.sort_values('rotation_score', ascending=False)\n",
    "    \n",
    "    print(f\"\\nPairs with BID ROTATION pattern (both win when competing):\")\n",
    "    print(f\"Found: {len(suspicious_df[suspicious_df['rotation_score'] > 0.3])} pairs with rotation > 30%\")\n",
    "    \n",
    "    for _, row in suspicious_df[suspicious_df['rotation_score'] > 0.3].head(10).iterrows():\n",
    "        print(f\"\\n  Co-bids: {row['co_bid_count']}, Rotation: {row['rotation_score']:.0%}\")\n",
    "        print(f\"    Bidder 1 wins: {row['b1_wins']}, Bidder 2 wins: {row['b2_wins']}, Other: {row['other_wins']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3altpzpcm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cartel groups using connected components\n",
    "# A cartel = group of bidders that frequently co-bid with each other\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# Build graph from frequent pairs\n",
    "G = nx.Graph()\n",
    "for _, row in frequent_pairs.iterrows():\n",
    "    G.add_edge(row['bidder_1'], row['bidder_2'], weight=row['co_bid_count'])\n",
    "\n",
    "print(f\"\\nCO-BIDDING NETWORK:\")\n",
    "print(f\"  Nodes (bidders): {G.number_of_nodes()}\")\n",
    "print(f\"  Edges (co-bid pairs): {G.number_of_edges()}\")\n",
    "\n",
    "# Find connected components (potential cartel groups)\n",
    "components = list(nx.connected_components(G))\n",
    "component_sizes = sorted([len(c) for c in components], reverse=True)\n",
    "\n",
    "print(f\"\\nCONNECTED COMPONENTS (potential cartels):\")\n",
    "print(f\"  Total groups: {len(components)}\")\n",
    "print(f\"  Largest group: {component_sizes[0]} bidders\")\n",
    "print(f\"  Groups with 3+ bidders: {len([s for s in component_sizes if s >= 3])}\")\n",
    "\n",
    "# Analyze largest components\n",
    "large_components = [c for c in components if len(c) >= 3]\n",
    "large_components = sorted(large_components, key=len, reverse=True)\n",
    "\n",
    "print(f\"\\nTOP 5 CARTEL CANDIDATES (groups of 3+ co-bidding suppliers):\")\n",
    "for i, comp in enumerate(large_components[:5]):\n",
    "    comp_list = list(comp)\n",
    "    \n",
    "    # Get subgraph\n",
    "    subgraph = G.subgraph(comp)\n",
    "    total_weight = sum(d['weight'] for _, _, d in subgraph.edges(data=True))\n",
    "    \n",
    "    print(f\"\\n  Group {i+1}: {len(comp)} bidders, {total_weight} total co-bids\")\n",
    "    \n",
    "    # Show bidder names (use suppliers table)\n",
    "    for bidder_id in comp_list[:5]:\n",
    "        name = suppliers[suppliers['supplier_id'] == bidder_id]['supplier_name'].values\n",
    "        name = name[0][:50] if len(name) > 0 else f'Bidder {bidder_id}'\n",
    "        print(f\"    - {name}\")\n",
    "    if len(comp_list) > 5:\n",
    "        print(f\"    ... and {len(comp_list) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sb9vtdn1ye",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize largest cartel candidate\n",
    "if len(large_components) > 0:\n",
    "    largest_cartel = large_components[0]\n",
    "    subgraph = G.subgraph(largest_cartel)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Layout\n",
    "    pos = nx.spring_layout(subgraph, k=2, iterations=50, seed=42)\n",
    "    \n",
    "    # Edge weights for width\n",
    "    edges = subgraph.edges(data=True)\n",
    "    weights = [d['weight'] for _, _, d in edges]\n",
    "    max_weight = max(weights) if weights else 1\n",
    "    edge_widths = [1 + 3 * w / max_weight for w in weights]\n",
    "    \n",
    "    # Draw\n",
    "    nx.draw_networkx_nodes(subgraph, pos, node_size=100, node_color='coral', alpha=0.8, ax=ax)\n",
    "    nx.draw_networkx_edges(subgraph, pos, width=edge_widths, alpha=0.5, edge_color='gray', ax=ax)\n",
    "    \n",
    "    ax.set_title(f'Largest Cartel Candidate: {len(largest_cartel)} Bidders')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/figures/hdbscan/cartel_network.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Saved: results/figures/hdbscan/cartel_network.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6qi1q0rf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cartel analysis results\n",
    "print(\"\\nSaving cartel analysis results...\")\n",
    "\n",
    "# Save co-bidding pairs\n",
    "frequent_pairs.to_csv('../results/cartel_co_bidding_pairs.csv', index=False)\n",
    "print(f\"Saved: results/cartel_co_bidding_pairs.csv ({len(frequent_pairs)} pairs)\")\n",
    "\n",
    "# Save cartel groups\n",
    "cartel_groups = []\n",
    "for i, comp in enumerate(large_components):\n",
    "    for bidder_id in comp:\n",
    "        cartel_groups.append({\n",
    "            'cartel_group': i,\n",
    "            'bidder_id': bidder_id,\n",
    "            'group_size': len(comp)\n",
    "        })\n",
    "\n",
    "cartel_df = pd.DataFrame(cartel_groups)\n",
    "if len(cartel_df) > 0:\n",
    "    # Merge with supplier names (bidder_id = supplier_id for winners)\n",
    "    cartel_df = cartel_df.merge(\n",
    "        suppliers[['supplier_id', 'supplier_name']].rename(columns={'supplier_id': 'bidder_id', 'supplier_name': 'name'}),\n",
    "        on='bidder_id', how='left'\n",
    "    )\n",
    "    cartel_df.to_csv('../results/cartel_groups.csv', index=False)\n",
    "    print(f\"Saved: results/cartel_groups.csv ({len(cartel_df)} bidders in {len(large_components)} groups)\")\n",
    "\n",
    "# Save suspicious pairs with rotation\n",
    "if len(suspicious_df) > 0:\n",
    "    suspicious_df.to_csv('../results/cartel_rotation_pairs.csv', index=False)\n",
    "    print(f\"Saved: results/cartel_rotation_pairs.csv ({len(suspicious_df)} pairs)\")\n",
    "else:\n",
    "    print(\"No bid rotation pairs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"HDBSCAN + CARTEL DETECTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDataset: {len(tenders):,} tenders\")\n",
    "print(f\"Sample: {SAMPLE_SIZE:,} ({SAMPLE_SIZE/len(tenders)*100:.1f}%)\")\n",
    "print(f\"Features: {len(feature_names)}\")\n",
    "print(f\"Parameters: min_cluster_size={MIN_CLUSTER_SIZE}, min_samples={MIN_SAMPLES}\")\n",
    "\n",
    "print(f\"\\nCLUSTERING:\")\n",
    "print(f\"  Clusters found: {n_clusters}\")\n",
    "print(f\"  Noise points: {n_noise:,} ({n_noise/len(labels)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nOUTLIER DETECTION (vs IF):\")\n",
    "print(f\"  Score correlation: {score_corr:.3f}\")\n",
    "print(f\"  Jaccard similarity: {jaccard:.3f}\")\n",
    "print(f\"  Both flagged: {both:,}\")\n",
    "print(f\"  Only HDBSCAN: {only_hdbscan:,}\")\n",
    "print(f\"  Only IF: {only_if:,}\")\n",
    "\n",
    "if len(suspicious_clusters) > 0:\n",
    "    print(f\"\\nSUSPICIOUS CLUSTERS: {len(suspicious_clusters)}\")\n",
    "    print(f\"  Total tenders in suspicious clusters: {suspicious_clusters['count'].sum():,}\")\n",
    "\n",
    "print(f\"\\nCARTEL DETECTION:\")\n",
    "print(f\"  Co-bidding network: {G.number_of_nodes()} bidders, {G.number_of_edges()} pairs\")\n",
    "print(f\"  Cartel groups (3+ bidders): {len(large_components)}\")\n",
    "if len(large_components) > 0:\n",
    "    print(f\"  Largest group: {len(large_components[0])} bidders\")\n",
    "if len(suspicious_df) > 0:\n",
    "    rotation_pairs = len(suspicious_df[suspicious_df['rotation_score'] > 0.3])\n",
    "    print(f\"  Pairs with bid rotation: {rotation_pairs}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "hdbscan_results.to_csv('../results/hdbscan_results.csv', index=False)\n",
    "print(f\"Saved: results/hdbscan_results.csv ({len(hdbscan_results):,} rows)\")\n",
    "\n",
    "# Save cluster info if available\n",
    "if hdbscan_detector.cluster_stats_ is not None and len(hdbscan_detector.cluster_stats_) > 0:\n",
    "    hdbscan_detector.cluster_stats_.to_csv('../results/hdbscan_clusters.csv')\n",
    "    print(f\"Saved: results/hdbscan_clusters.csv ({len(hdbscan_detector.cluster_stats_)} clusters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## Висновки\n",
    "\n",
    "### HDBSCAN дає:\n",
    "1. **Outlier scores (1 - probability)** — замінює LOF для ensemble\n",
    "   - Noise points мають score = 1.0 (найвищий)\n",
    "   - Точки в щільних кластерах мають score близький до 0\n",
    "2. **Кластери** — групи схожих тендерів\n",
    "3. **Noise points** — точки, що не вписуються в жоден кластер (80%)\n",
    "\n",
    "### Cartel Detection:\n",
    "1. **Co-bidding network** — граф постачальників, що беруть участь разом\n",
    "2. **Connected components** — потенційні картельні групи\n",
    "3. **Bid rotation** — пари, що виграють по черзі (підозріло!)\n",
    "\n",
    "### Для подальшого аналізу:\n",
    "- `cartel_groups.csv` — групи постачальників для розслідування\n",
    "- `cartel_rotation_pairs.csv` — пари з патерном bid rotation\n",
    "- `cartel_co_bidding_pairs.csv` — всі пари, що часто бідують разом\n",
    "\n",
    "### Для Ensemble:\n",
    "- Використовувати `hdbscan_outlier_score` (1 - probability)\n",
    "- Або `hdbscan_is_noise` як бінарний flag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}