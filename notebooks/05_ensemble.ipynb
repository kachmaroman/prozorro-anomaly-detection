{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Ensemble Anomaly Detection\n",
    "\n",
    "**Мета:** Об'єднати результати всіх методів в єдиний risk score.\n",
    "\n",
    "**Методи (5 рівнів):**\n",
    "1. Rule-based (44 rules) — процедурні порушення\n",
    "2. Statistical (Benford, Z-score) — числові аномалії\n",
    "3. Isolation Forest — глобальні outliers\n",
    "4. HDBSCAN — кластеризація + outlier detection\n",
    "5. Network Analysis — мережевий аналіз (картелі, монополії)\n",
    "\n",
    "**Ensemble підхід:**\n",
    "- Weighted voting: кожен метод голосує з вагою\n",
    "- Consensus: скільки методів flagged тендер\n",
    "- Final risk score: нормалізована комбінація\n",
    "\n",
    "**Pipeline:**\n",
    "1. Rule-based ✓\n",
    "2. Statistical ✓\n",
    "3. Isolation Forest ✓\n",
    "4. HDBSCAN ✓\n",
    "5. Network ✓\n",
    "6. **Ensemble** ← current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: YEARS=[2022, 2023, 2024, 2025], CONTAMINATION=0.05\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data_loader import load_tenders, load_bids, load_buyers, load_suppliers, load_bidders\n",
    "from src.detectors import (\n",
    "    RuleBasedDetector,\n",
    "    StatisticalDetector,\n",
    "    PyODDetector,\n",
    "    HDBSCANDetector,\n",
    "    NetworkAnalysisDetector,\n",
    "    EnsembleDetector,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "YEARS = [2022, 2023, 2024, 2025]\n",
    "CONTAMINATION = 0.05\n",
    "HDBSCAN_SAMPLE_SIZE = 1_000_000  # HDBSCAN on sample\n",
    "# ============================================================\n",
    "\n",
    "# Create output directories\n",
    "Path('../results/figures/ensemble').mkdir(parents=True, exist_ok=True)\n",
    "Path('../results').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(f\"Configuration: YEARS={YEARS}, CONTAMINATION={CONTAMINATION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Завантаження даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Scanning 2022...\n",
      "Scanning 2023...\n",
      "Scanning 2024...\n",
      "Scanning 2025...\n",
      "Loaded 12,877,960 records\n",
      "Scanning bids 2022...\n",
      "Scanning bids 2023...\n",
      "Scanning bids 2024...\n",
      "Scanning bids 2025...\n",
      "Loaded 2,639,473 bids\n",
      "Loaded buyers: 35,995\n",
      "Loaded suppliers: 358,376\n",
      "Loaded bidders: 72,291\n",
      "\n",
      "Dataset:\n",
      "  Tenders: 12,877,960\n",
      "  Bids: 2,639,473\n",
      "  Buyers: 35,995\n",
      "  Suppliers: 358,376\n",
      "  Bidders: 72,291\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "tenders = load_tenders(years=YEARS)\n",
    "bids = load_bids(years=YEARS)\n",
    "buyers = load_buyers()\n",
    "suppliers = load_suppliers()\n",
    "bidders = load_bidders()\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Tenders: {len(tenders):,}\")\n",
    "print(f\"  Bids: {len(bids):,}\")\n",
    "print(f\"  Buyers: {len(buyers):,}\")\n",
    "print(f\"  Suppliers: {len(suppliers):,}\")\n",
    "print(f\"  Bidders: {len(bidders):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Run All Detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. RULE-BASED DETECTOR\n",
      "============================================================\n",
      "Processing 12,877,960 tenders...\n",
      "Step 1/4: Computing aggregations...\n",
      "  Computing CPV stats...\n",
      "  Computing buyer stats...\n",
      "  Computing supplier stats...\n",
      "  Computing pair stats...\n",
      "  Aggregations complete.\n",
      "Step 2/4: Merging reference data...\n",
      "Step 3/4: Applying 44 rules...\n"
     ]
    }
   ],
   "source": [
    "# 1. Rule-based\n",
    "print(\"=\"*60)\n",
    "print(\"1. RULE-BASED DETECTOR\")\n",
    "print(\"=\"*60)\n",
    "rule_detector = RuleBasedDetector()\n",
    "rule_results = rule_detector.detect(tenders, buyers_df=buyers, bids_df=bids)\n",
    "print(f\"\\nRule-based anomalies (score >= 6): {(rule_results['rule_risk_score'] >= 6).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Statistical\n",
    "print(\"=\"*60)\n",
    "print(\"2. STATISTICAL DETECTOR\")\n",
    "print(\"=\"*60)\n",
    "stat_detector = StatisticalDetector()\n",
    "stat_results = stat_detector.detect(tenders, bids_df=bids)\n",
    "print(f\"\\nStatistical anomalies (score >= 3): {(stat_results['stat_score'] >= 3).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Isolation Forest (via PyODDetector)\n",
    "print(\"=\"*60)\n",
    "print(\"3. ISOLATION FOREST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if_detector = PyODDetector(\n",
    "    algorithm=\"iforest\",\n",
    "    contamination=CONTAMINATION,\n",
    "    random_state=42,\n",
    ")\n",
    "if_results = if_detector.fit_detect(tenders, buyers_df=buyers, suppliers_df=suppliers)\n",
    "\n",
    "# Rename columns for ensemble compatibility\n",
    "if_results = if_results.rename(columns={\"score\": \"if_score\", \"anomaly\": \"if_anomaly\"})\n",
    "\n",
    "print(f\"IF anomalies: {if_results['if_anomaly'].sum():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. HDBSCAN (on sample)\n",
    "print(\"=\"*60)\n",
    "print(\"4. HDBSCAN (sample)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "hdbscan_detector = HDBSCANDetector(\n",
    "    min_cluster_size=50,\n",
    "    min_samples=10,\n",
    "    contamination=CONTAMINATION,\n",
    ")\n",
    "hdbscan_results = hdbscan_detector.fit_detect(\n",
    "    tenders, \n",
    "    buyers_df=buyers, \n",
    "    suppliers_df=suppliers,\n",
    "    sample_size=HDBSCAN_SAMPLE_SIZE,\n",
    ")\n",
    "print(f\"HDBSCAN anomalies: {hdbscan_results['hdbscan_anomaly'].sum():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Network Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"5. NETWORK ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "network_detector = NetworkAnalysisDetector(\n",
    "    min_co_bids=3,\n",
    "    min_contracts=3,\n",
    "    min_community_size=3,\n",
    "    # Stricter anomaly thresholds to reduce false positives\n",
    "    suspicious_min_degree=10,        # Was 5\n",
    "    suspicious_min_clustering=0.7,   # Was 0.5\n",
    "    rotation_min_ratio=0.7,          # Was 0.5\n",
    "    rotation_min_interactions=5,     # New: at least 5 head-to-head competitions\n",
    "    monopoly_min_ratio=0.9,          # Was 0.8\n",
    "    monopoly_min_contracts=20,       # Was 10\n",
    ")\n",
    "network_results = network_detector.fit_detect(tenders, bids_df=bids, bidders_df=bidders)\n",
    "print(f\"\\nNetwork anomalies: {network_results['network_anomaly'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results using EnsembleDetector\n",
    "print(\"=\"*60)\n",
    "print(\"COMBINING RESULTS WITH ENSEMBLE DETECTOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ensemble_detector = EnsembleDetector(\n",
    "    weights={\n",
    "        'rule': 1.0,\n",
    "        'stat': 0.8,\n",
    "        'if': 1.0,\n",
    "        'hdbscan': 0.8,\n",
    "        'network': 1.0,\n",
    "    },\n",
    "    consensus_threshold=2,\n",
    ")\n",
    "\n",
    "ensemble = ensemble_detector.combine(\n",
    "    rule_results=rule_results,\n",
    "    stat_results=stat_results,\n",
    "    if_results=if_results,\n",
    "    hdbscan_results=hdbscan_results,\n",
    "    network_results=network_results,\n",
    ")\n",
    "\n",
    "print(f\"\\nEnsemble dataset: {len(ensemble):,} tenders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "# Skip - EnsembleDetector handles normalization internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores are already normalized by EnsembleDetector\n",
    "print(\"Score columns in ensemble results:\")\n",
    "print([col for col in ensemble.columns if 'score' in col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Compute Ensemble Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EnsembleDetector computes scores and consensus automatically\n",
    "# ensemble_score, consensus_count, consensus_pct, ensemble_anomaly, ensemble_risk_level\n",
    "\n",
    "print(\"WEIGHTS:\", ensemble_detector.weights)\n",
    "print(f\"\\nEnsemble score stats:\")\n",
    "print(ensemble['ensemble_score'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk levels are computed by EnsembleDetector\n",
    "# Based on consensus_count: critical (>=75%), high (>=50%), medium (>=25%), low (<25%)\n",
    "\n",
    "risk_dist = ensemble['ensemble_risk_level'].value_counts()\n",
    "print(\"\\nENSEMBLE RISK DISTRIBUTION:\")\n",
    "for level in ['critical', 'high', 'medium', 'low']:\n",
    "    count = risk_dist.get(level, 0)\n",
    "    pct = count / len(ensemble) * 100\n",
    "    print(f\"  {level:10} {count:>10,} ({pct:>5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Consensus Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consensus breakdown (5 methods)\n",
    "n_methods = len(ensemble_detector.methods_used)\n",
    "print(f\"CONSENSUS BREAKDOWN ({n_methods} methods: {ensemble_detector.methods_used}):\")\n",
    "\n",
    "consensus_dist = ensemble['consensus_count'].value_counts().sort_index()\n",
    "for count, num in consensus_dist.items():\n",
    "    pct = num / len(ensemble) * 100\n",
    "    methods = f\"{count}/{n_methods} methods\"\n",
    "    print(f\"  {methods}: {num:>10,} ({pct:>5.2f}%)\")\n",
    "\n",
    "# Critical: most methods agree\n",
    "critical_threshold = int(n_methods * 0.75)  # 75%+ methods\n",
    "critical_consensus = ensemble[ensemble['consensus_count'] >= critical_threshold]\n",
    "print(f\"\\nCRITICAL ({critical_threshold}+ of {n_methods} methods): {len(critical_consensus):,} tenders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize consensus\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Consensus distribution\n",
    "n_methods = len(ensemble_detector.methods_used)\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, n_methods + 1))\n",
    "axes[0].bar(consensus_dist.index, consensus_dist.values, color=colors[:len(consensus_dist)])\n",
    "axes[0].set_xlabel('Number of Methods Flagged')\n",
    "axes[0].set_ylabel('Number of Tenders')\n",
    "axes[0].set_title(f'Consensus Distribution ({n_methods} methods)')\n",
    "axes[0].set_xticks(range(n_methods + 1))\n",
    "\n",
    "# Risk level pie\n",
    "risk_colors = {'critical': '#d62728', 'high': '#ff7f0e', 'medium': '#ffbb78', 'low': '#2ca02c'}\n",
    "risk_order = ['critical', 'high', 'medium', 'low']\n",
    "risk_values = [risk_dist.get(r, 0) for r in risk_order]\n",
    "axes[1].pie(risk_values, labels=risk_order, colors=[risk_colors[r] for r in risk_order],\n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Ensemble Risk Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/ensemble/consensus_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. Method Contribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method contribution analysis using EnsembleDetector\n",
    "print(\"METHOD STATISTICS:\")\n",
    "print(ensemble_detector.method_summary().to_string(index=False))\n",
    "\n",
    "# Correlation matrix\n",
    "print(\"\\nSCORE CORRELATION MATRIX:\")\n",
    "corr_matrix = ensemble_detector.correlation_matrix()\n",
    "print(corr_matrix.round(3).to_string())\n",
    "\n",
    "# Agreement matrix (Jaccard similarity)\n",
    "print(\"\\nANOMALY AGREEMENT MATRIX (Jaccard):\")\n",
    "agreement_matrix = ensemble_detector.agreement_matrix()\n",
    "print(agreement_matrix.round(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize agreement matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Correlation heatmap\n",
    "ax = axes[0]\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax)\n",
    "ax.set_title('Score Correlation Matrix')\n",
    "\n",
    "# Agreement heatmap (Jaccard)\n",
    "ax = axes[1]\n",
    "sns.heatmap(agreement_matrix.astype(float), annot=True, fmt='.2f', cmap='YlOrRd', ax=ax)\n",
    "ax.set_title('Anomaly Agreement Matrix (Jaccard)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/ensemble/method_agreement.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 8. Critical Tenders Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get critical tenders (using EnsembleDetector helper)\n",
    "critical_tenders = ensemble_detector.get_critical_tenders()\n",
    "normal_tenders = ensemble[ensemble['consensus_count'] == 0]\n",
    "\n",
    "# Merge with tender details\n",
    "critical_with_details = critical_tenders.merge(\n",
    "    tenders[['tender_id', 'tender_value', 'is_single_bidder', 'is_competitive']],\n",
    "    on='tender_id', how='left'\n",
    ")\n",
    "normal_with_details = normal_tenders.merge(\n",
    "    tenders[['tender_id', 'tender_value', 'is_single_bidder', 'is_competitive']],\n",
    "    on='tender_id', how='left'\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CRITICAL TENDERS CHARACTERISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'Critical':>15} {'Normal':>15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Value\n",
    "c_mean = critical_with_details['tender_value'].mean()\n",
    "n_mean = normal_with_details['tender_value'].mean()\n",
    "print(f\"{'Mean tender value (M UAH)':<30} {c_mean/1e6:>15,.2f} {n_mean/1e6:>15,.2f}\")\n",
    "\n",
    "c_med = critical_with_details['tender_value'].median()\n",
    "n_med = normal_with_details['tender_value'].median()\n",
    "print(f\"{'Median tender value (K UAH)':<30} {c_med/1e3:>15,.1f} {n_med/1e3:>15,.1f}\")\n",
    "\n",
    "# Competition\n",
    "c_sb = critical_with_details['is_single_bidder'].mean()\n",
    "n_sb = normal_with_details['is_single_bidder'].mean()\n",
    "print(f\"{'Single bidder rate (%)':<30} {c_sb*100:>15.1f} {n_sb*100:>15.1f}\")\n",
    "\n",
    "c_comp = critical_with_details['is_competitive'].mean()\n",
    "n_comp = normal_with_details['is_competitive'].mean()\n",
    "print(f\"{'Competitive rate (%)':<30} {c_comp*100:>15.1f} {n_comp*100:>15.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procurement method distribution\n",
    "print(\"\\nPROCUREMENT METHOD:\")\n",
    "\n",
    "# Merge with tenders to get procurement method\n",
    "critical_methods = critical_tenders.merge(tenders[['tender_id', 'procurement_method']], on='tender_id')\n",
    "normal_methods = normal_tenders.merge(tenders[['tender_id', 'procurement_method']], on='tender_id')\n",
    "\n",
    "critical_method_dist = critical_methods['procurement_method'].value_counts(normalize=True) * 100\n",
    "normal_method_dist = normal_methods['procurement_method'].value_counts(normalize=True) * 100\n",
    "\n",
    "for method in critical_method_dist.index:\n",
    "    c_pct = critical_method_dist.get(method, 0)\n",
    "    n_pct = normal_method_dist.get(method, 0)\n",
    "    ratio = c_pct / n_pct if n_pct > 0 else 0\n",
    "    print(f\"  {method}: {c_pct:.1f}% (vs {n_pct:.1f}% normal) - {ratio:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year distribution\n",
    "print(\"\\nYEAR DISTRIBUTION:\")\n",
    "\n",
    "critical_years = critical_tenders.merge(tenders[['tender_id', 'year']], on='tender_id')\n",
    "critical_year_dist = critical_years['year'].value_counts().sort_index()\n",
    "\n",
    "for year, count in critical_year_dist.items():\n",
    "    total_year = len(tenders[tenders['year'] == year])\n",
    "    pct = count / total_year * 100\n",
    "    print(f\"  {year}: {count:,} ({pct:.2f}% of year)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 9. Top Risky Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top buyers by critical tender count\n",
    "print(\"TOP 10 BUYERS BY CRITICAL TENDERS:\")\n",
    "\n",
    "critical_full = critical_tenders.merge(\n",
    "    tenders[['tender_id', 'buyer_id', 'supplier_id', 'tender_value']], \n",
    "    on='tender_id', how='left'\n",
    ")\n",
    "\n",
    "top_buyers_critical = critical_full.groupby('buyer_id').agg({\n",
    "    'tender_id': 'count',\n",
    "    'tender_value': 'sum',\n",
    "    'ensemble_score': 'mean'\n",
    "}).sort_values('tender_id', ascending=False).head(10)\n",
    "\n",
    "top_buyers_critical = top_buyers_critical.reset_index().merge(\n",
    "    buyers[['buyer_id', 'buyer_name', 'buyer_region']], on='buyer_id', how='left'\n",
    ")\n",
    "\n",
    "for _, row in top_buyers_critical.iterrows():\n",
    "    name = str(row['buyer_name'])[:50] if pd.notna(row['buyer_name']) else 'N/A'\n",
    "    print(f\"  {row['tender_id']:>5,} tenders | {row['tender_value']/1e6:>10,.1f}M UAH | {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top suppliers by critical tender count\n",
    "print(\"\\nTOP 10 SUPPLIERS BY CRITICAL TENDERS:\")\n",
    "\n",
    "top_suppliers_critical = critical_full.groupby('supplier_id').agg({\n",
    "    'tender_id': 'count',\n",
    "    'tender_value': 'sum',\n",
    "    'ensemble_score': 'mean'\n",
    "}).sort_values('tender_id', ascending=False).head(10)\n",
    "\n",
    "top_suppliers_critical = top_suppliers_critical.reset_index().merge(\n",
    "    suppliers[['supplier_id', 'supplier_name']], on='supplier_id', how='left'\n",
    ")\n",
    "\n",
    "for _, row in top_suppliers_critical.iterrows():\n",
    "    name = str(row['supplier_name'])[:50] if pd.notna(row['supplier_name']) else 'N/A'\n",
    "    print(f\"  {row['tender_id']:>5,} tenders | {row['tender_value']/1e6:>10,.1f}M UAH | {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ensemble results\n",
    "print(\"Saving results...\")\n",
    "\n",
    "# Define output columns\n",
    "score_cols = [col for col in ensemble.columns if 'score' in col]\n",
    "anomaly_cols = [col for col in ensemble.columns if 'anomaly' in col]\n",
    "output_cols = ['tender_id'] + score_cols + anomaly_cols + ['consensus_count', 'consensus_pct', 'ensemble_risk_level']\n",
    "\n",
    "# Save full results\n",
    "ensemble[output_cols].to_csv('../results/ensemble_results.csv', index=False)\n",
    "print(f\"Saved full results: results/ensemble_results.csv ({len(ensemble):,} rows)\")\n",
    "\n",
    "# Save critical only\n",
    "critical_tenders[output_cols].to_csv('../results/critical_tenders.csv', index=False)\n",
    "print(f\"Saved critical tenders: results/critical_tenders.csv ({len(critical_tenders):,} rows)\")\n",
    "\n",
    "# Save summary\n",
    "summary = ensemble_detector.summary()\n",
    "summary.to_csv('../results/ensemble_summary.csv', index=False)\n",
    "print(f\"Saved summary: results/ensemble_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ENSEMBLE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDataset: {len(ensemble):,} tenders ({YEARS[0]}-{YEARS[-1]})\")\n",
    "print(f\"\\nMethods combined ({len(ensemble_detector.methods_used)}):\")\n",
    "for method in ensemble_detector.methods_used:\n",
    "    anomaly_col = f\"{method}_anomaly\"\n",
    "    if anomaly_col in ensemble.columns:\n",
    "        count = ensemble[anomaly_col].sum()\n",
    "        print(f\"  {method}: {count:,} anomalies\")\n",
    "\n",
    "print(f\"\\nENSEMBLE RISK LEVELS:\")\n",
    "for level in ['critical', 'high', 'medium', 'low']:\n",
    "    count = risk_dist.get(level, 0)\n",
    "    pct = count / len(ensemble) * 100\n",
    "    print(f\"  {level:10} {count:>10,} ({pct:>5.2f}%)\")\n",
    "\n",
    "print(f\"\\nCRITICAL TENDERS (ensemble_risk_level == 'critical'):\")\n",
    "print(f\"  Count: {len(critical_tenders):,}\")\n",
    "if len(critical_with_details) > 0:\n",
    "    print(f\"  Total value: {critical_with_details['tender_value'].sum()/1e9:.2f}B UAH\")\n",
    "    print(f\"  Mean value: {critical_with_details['tender_value'].mean()/1e6:.2f}M UAH\")\n",
    "    print(f\"  Single bidder rate: {critical_with_details['is_single_bidder'].mean()*100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## Висновки\n",
    "\n",
    "### Ensemble підхід (5 методів):\n",
    "- **Rule-based** — процедурні порушення (44 rules)\n",
    "- **Statistical** — числові аномалії (Benford, Z-score, HHI)\n",
    "- **Isolation Forest** — глобальні outliers\n",
    "- **HDBSCAN** — кластеризація + outlier detection\n",
    "- **Network Analysis** — картелі, монополії, bid rotation\n",
    "\n",
    "### Scoring:\n",
    "- **Weighted score**: зважена комбінація нормалізованих scores\n",
    "- **Consensus voting**: кількість методів, що flagged тендер\n",
    "- **Risk levels**: critical (>=75%), high (>=50%), medium (>=25%), low (<25%)\n",
    "\n",
    "### Ключові результати:\n",
    "- **Critical** (більшість методів згодні) — найвища впевненість для аудиту\n",
    "- Методи доповнюють один одного (низька кореляція)\n",
    "- Різні типи аномалій покриваються різними методами\n",
    "\n",
    "### Збережені файли:\n",
    "- `results/ensemble_results.csv` — всі тендери з scores\n",
    "- `results/critical_tenders.csv` — лише critical\n",
    "- `results/ensemble_summary.csv` — summary statistics\n",
    "\n",
    "### EnsembleDetector:\n",
    "```python\n",
    "from src.detectors import EnsembleDetector\n",
    "\n",
    "detector = EnsembleDetector(consensus_threshold=2)\n",
    "results = detector.combine(\n",
    "    rule_results=...,\n",
    "    stat_results=...,\n",
    "    if_results=...,\n",
    "    hdbscan_results=...,\n",
    "    network_results=...,\n",
    ")\n",
    "\n",
    "critical = detector.get_critical_tenders()\n",
    "print(detector.summary())\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
